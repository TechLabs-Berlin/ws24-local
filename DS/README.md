#Readme

Once the data set was finalized. It was up to the DS techies to clean the dataset and make amendments in the dataset that makes it usable for the project.  Initially, the raw data consisted of the shape (1901, 11) with 1901 rows and 11 columns. As some of the columns did not make any sense for the project such as the ‘audio’ links of the artists, ‘title_and_location’ column was a merger of the ‘venue’ and ‘artist’ column therefore they were dropped to make to reduce the number of columns and the complexity in the dataset. Moreover, the ‘home’ column was separated into different columns namely ‘city’ and the ‘state’ to simplify the search centred around location as well as to create city based filters for the users. Apart from these, many ‘NaN’ value based rows were removed from the dataset as these were causing a hindrance for the search tool. 
So after data cleaning and modification the data was ready to be created for filtration. The modified data had a shape of (1068, 9) with 1068 rows and 9 rows. Genre filters were created to show the users specifically those concerts based on their taste of music they usually listen to such as Electronic, Pop, Hip-Hop etc. Users may filter out results based on the city that they live in to find out the local music scene happening in their city or might want to visit nearby cities to attend any concert of their favourite artist. And finally, the ticket price filter serves as the combination of price brackets that the user may want see, which helps them to reach a decision of whether to buy a concert ticket or not. 
Additionally, since we wanted to add a feature of filtering based on the tickets entry price of the concerts, hence random integer values were created for the concerts and assigned to the individuals rows so that price based filters could be created and the user may use them to base on their willingness to pay.  An added bonus feature initially thought of was to add ratings to the concerts based on the users past experiences as well as to provide recommendations, therefore these had also been added to the dataset. 
However, in order to graduate integrating a Machine Learning algorithm was a need and this motivated us to look for ways to implement it into our project. The team came across the decision to create a recommendation system based on the data set to the users where they may see similar concerts apart from their search. To get started with ML algos, our track lead suggested to either go for collaborative filtering based or content based recommendation system, which led us to research deeper into backend working of a recommendation system and coming up with a content based recommendation ML algo that fits the bill according to our dataset. For deploying our content based recommendation system we first had to extract the relevant features from the dataset that could be used for the analysis such as the genre, artist, venue etc. Then we used the Scikit TF-IDF (Term Frequency-Inverse Document Frequency) vectorization to convert our features into number and the amount of times these occur in our dataset. The cosine similarity function calculates this similarity scores among the concerts and gives an output of the concerts that are similar in nature (features) such is an example of concerts that are taking place on the same venue with comparable genre, city etc. and finally generate recommendations based on the similarity scores.
